{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDCC HW: Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Implement a MapReduce job that identifies the 100 most followed users in the dataset.\n",
    "\n",
    "Hint: We are not interested in creating lists of followers here. We just need to count the followers of each user in the Reduce phase. This is called the in-degree of a user. Moreover, a temporary data structure D of fixed 100 positions is required. This structure will be initially filled with the first 100 users that are processed in the Reduce phase. Then, the next users (101, 102, 103â€¦) will replace a user in D, only if their in-degree is greater than the in-degree of the least-followed user in D. Notice that the ideal data structure for D is a min-heap (https://docs.python.org/3/library/heapq.html). However, it is totally acceptable to appropriately use a data structure such as a dictionary, or a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file src/task3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import heapq\n",
    "\n",
    "TOP_FOLLOWERS = 100\n",
    "\n",
    "# Implement a MapReduce job that creates a list of followees for each user in the dataset.\n",
    "class MostFollowed(MRJob):\n",
    "\n",
    "    # Arg 1: self: the class itself (this)\n",
    "    # Arg 2: Input key to the map function (here:none)\n",
    "    # Arg 3: Input value to the map function (here:one line from the input file)\n",
    "    def mapper(self, _, line):\n",
    "        # yield (followee, 1) pair\n",
    "        (follower, followee) = line.split()\n",
    "        yield(followee, 1)\n",
    "\n",
    "\n",
    "    def combiner(self, followee, follower_count):\n",
    "        # yield (followee, sum of followers)\n",
    "        yield(followee, sum(follower_count))\n",
    "\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.heap = []\n",
    "\n",
    "\n",
    "    # Arg 1: self: the class itself (this)\n",
    "    # Arg 2: Input key to the reduce function (here: the key that was emitted by the mapper)\n",
    "    # Arg 3: Input value to the reduce function (here: a generator object; something like a\n",
    "    # sorted list of ALL values associated with the same key)\n",
    "    def reducer(self, followee, follower_count):\n",
    "        heapq.heappush(self.heap, (sum(follower_count), followee))\n",
    "        \n",
    "        if len(self.heap) > TOP_FOLLOWERS:\n",
    "            heapq.heappop(self.heap)\n",
    "\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for (follower_count, followee) in self.heap:\n",
    "            yield (followee, follower_count)\n",
    "\n",
    "\n",
    "    # Step 2: Run the TOP_FOLLOWERS\n",
    "    # The mapper outputs \"TOP_FOLLOWERS\" as the key and (follower_count, followee) as value\n",
    "    # Put the count as key so it can be used directly as input to heapq.nlargest()\n",
    "    def top_mapper(self, followee, follower_count):\n",
    "       yield (str(TOP_FOLLOWERS), (follower_count, followee))\n",
    "\n",
    "\n",
    "    # The finds the largest of the values.\n",
    "    def top_reducer(self ,_, follower_counts):\n",
    "        for follower_count in heapq.nlargest(TOP_FOLLOWERS, follower_counts):\n",
    "            yield (follower_count[1], follower_count[0])\n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final\n",
    "                   ),\n",
    "\n",
    "            MRStep(mapper=self.top_mapper,\n",
    "                   reducer=self.top_reducer) \n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostFollowed.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in Standalone Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 src/task3.py data/graph.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 src/task3.py -r hadoop data/graph.txt -o task3_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the output from HDFS to local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -copyToLocal task3_output /home/bdccuser/bdcc-assignment1/output/task3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python364jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.6.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}