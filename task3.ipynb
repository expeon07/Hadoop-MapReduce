{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDCC HW: Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Implement a MapReduce job that identifies the 100 most followed users in the dataset.\n",
    "\n",
    "Hint: We are not interested in creating lists of followers here. We just need to count the followers of each user in the Reduce phase. This is called the in-degree of a user. Moreover, a temporary data structure D of fixed 100 positions is required. This structure will be initially filled with the first 100 users that are processed in the Reduce phase. Then, the next users (101, 102, 103â€¦) will replace a user in D, only if their in-degree is greater than the in-degree of the least-followed user in D. Notice that the ideal data structure for D is a min-heap (https://docs.python.org/3/library/heapq.html). However, it is totally acceptable to appropriately use a data structure such as a dictionary, or a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/task3.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/task3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import heapq\n",
    "\n",
    "TOP_FOLLOWERS = 100\n",
    "\n",
    "# Implement a MapReduce job that creates a list of followees for each user in the dataset.\n",
    "class MostFollowed(MRJob):\n",
    "\n",
    "    # Arg 1: self: the class itself (this)\n",
    "    # Arg 2: Input key to the map function (here:none)\n",
    "    # Arg 3: Input value to the map function (here:one line from the input file)\n",
    "    def mapper(self, _, line):\n",
    "        # yield (followee, 1) pair\n",
    "        (follower, followee) = line.split()\n",
    "        yield(followee, 1)\n",
    "\n",
    "\n",
    "    def combiner(self, followee, follower_count):\n",
    "        # yield (followee, sum of followers)\n",
    "        yield(followee, sum(follower_count))\n",
    "\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.heap = []\n",
    "\n",
    "\n",
    "    # Arg 1: self: the class itself (this)\n",
    "    # Arg 2: Input key to the reduce function (here: the key that was emitted by the mapper)\n",
    "    # Arg 3: Input value to the reduce function (here: a generator object; something like a\n",
    "    # sorted list of ALL values associated with the same key)\n",
    "    def reducer(self, followee, follower_count):\n",
    "        heapq.heappush(self.heap, (sum(follower_count), followee))\n",
    "        \n",
    "        if len(self.heap) > TOP_FOLLOWERS:\n",
    "            heapq.heappop(self.heap)\n",
    "\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for (follower_count, followee) in self.heap:\n",
    "            yield (followee, follower_count)\n",
    "\n",
    "\n",
    "    # Step 2: Run the TOP_FOLLOWERS\n",
    "    # The mapper outputs \"TOP_FOLLOWERS\" as the key and (follower_count, followee) as value\n",
    "    # Put the count as key so it can be used directly as input to heapq.nlargest()\n",
    "    def top_mapper(self, followee, follower_count):\n",
    "       yield (str(TOP_FOLLOWERS), (follower_count, followee))\n",
    "\n",
    "\n",
    "    # The finds the largest of the values.\n",
    "    def top_reducer(self ,_, follower_counts):\n",
    "        for follower_count in heapq.nlargest(TOP_FOLLOWERS, follower_counts):\n",
    "            yield (follower_count[1], follower_count[0])\n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final\n",
    "                   ),\n",
    "\n",
    "            MRStep(mapper=self.top_mapper,\n",
    "                   reducer=self.top_reducer) \n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostFollowed.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in Standalone Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task3.bdccuser.20210608.111609.612780\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task3.bdccuser.20210608.111609.612780/output\n",
      "Streaming final output from /tmp/task3.bdccuser.20210608.111609.612780/output...\n",
      "\"663931\"\t4256\n",
      "\"482709\"\t3137\n",
      "\"663560\"\t2602\n",
      "\"357531\"\t1970\n",
      "\"1034018\"\t1599\n",
      "\"664789\"\t1592\n",
      "\"663606\"\t1491\n",
      "\"155432\"\t963\n",
      "\"622420\"\t956\n",
      "\"280436\"\t907\n",
      "\"115241\"\t871\n",
      "\"665990\"\t836\n",
      "\"664320\"\t828\n",
      "\"115674\"\t812\n",
      "\"1120568\"\t798\n",
      "\"650922\"\t787\n",
      "\"666763\"\t785\n",
      "\"681308\"\t716\n",
      "\"402217\"\t698\n",
      "\"515984\"\t692\n",
      "\"108624\"\t664\n",
      "\"682469\"\t663\n",
      "\"681398\"\t639\n",
      "\"670310\"\t625\n",
      "\"682734\"\t618\n",
      "\"663683\"\t609\n",
      "\"280408\"\t607\n",
      "\"670353\"\t576\n",
      "\"655586\"\t572\n",
      "\"663571\"\t548\n",
      "\"401877\"\t543\n",
      "\"667359\"\t538\n",
      "\"670923\"\t521\n",
      "\"1032030\"\t503\n",
      "\"17405\"\t501\n",
      "\"663852\"\t499\n",
      "\"663534\"\t499\n",
      "\"656427\"\t499\n",
      "\"670279\"\t494\n",
      "\"254927\"\t492\n",
      "\"1095899\"\t491\n",
      "\"667116\"\t481\n",
      "\"642336\"\t465\n",
      "\"678519\"\t458\n",
      "\"663579\"\t458\n",
      "\"480826\"\t446\n",
      "\"105478\"\t428\n",
      "\"686285\"\t422\n",
      "\"697499\"\t417\n",
      "\"1030566\"\t404\n",
      "\"1108861\"\t401\n",
      "\"1014440\"\t400\n",
      "\"666986\"\t399\n",
      "\"663619\"\t378\n",
      "\"685162\"\t377\n",
      "\"1067598\"\t375\n",
      "\"1064471\"\t364\n",
      "\"683704\"\t355\n",
      "\"663548\"\t350\n",
      "\"656431\"\t350\n",
      "\"666653\"\t345\n",
      "\"682680\"\t337\n",
      "\"323007\"\t331\n",
      "\"681748\"\t330\n",
      "\"114922\"\t330\n",
      "\"665647\"\t328\n",
      "\"707743\"\t323\n",
      "\"93931\"\t314\n",
      "\"62876\"\t311\n",
      "\"639586\"\t310\n",
      "\"94027\"\t308\n",
      "\"23463\"\t307\n",
      "\"1024068\"\t305\n",
      "\"683057\"\t302\n",
      "\"670343\"\t302\n",
      "\"666666\"\t301\n",
      "\"8792\"\t299\n",
      "\"664242\"\t293\n",
      "\"681623\"\t289\n",
      "\"199209\"\t287\n",
      "\"672641\"\t285\n",
      "\"106355\"\t285\n",
      "\"20934\"\t284\n",
      "\"1020498\"\t281\n",
      "\"683230\"\t280\n",
      "\"44006\"\t280\n",
      "\"666758\"\t279\n",
      "\"111468\"\t279\n",
      "\"1038591\"\t279\n",
      "\"684652\"\t278\n",
      "\"667325\"\t278\n",
      "\"716475\"\t276\n",
      "\"1099803\"\t275\n",
      "\"666717\"\t271\n",
      "\"21352\"\t270\n",
      "\"682140\"\t265\n",
      "\"705285\"\t264\n",
      "\"268337\"\t264\n",
      "\"1021166\"\t263\n",
      "\"666701\"\t262\n",
      "Removing temp directory /tmp/task3.bdccuser.20210608.111609.612780...\n"
     ]
    }
   ],
   "source": [
    "!python3 src/task3.py data/graph.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/hdoop/hadoop-3.2.1/bin...\n",
      "Found hadoop binary: /home/hdoop/hadoop-3.2.1/bin/hadoop\n",
      "Using Hadoop version 3.2.1\n",
      "Looking for Hadoop streaming jar in /home/hdoop/hadoop-3.2.1...\n",
      "Found Hadoop streaming jar: /home/hdoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar\n",
      "Creating temp directory /tmp/task3.bdccuser.20210608.112139.337160\n",
      "uploading working dir files to hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210608.112139.337160/files/wd...\n",
      "Copying other local files to hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210608.112139.337160/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6352124431624138651/] [] /tmp/streamjob4544271477495990304.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bdccuser/.staging/job_1623140774028_0008\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Total input files to process : 1\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  number of splits:2\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Submitting tokens for job: job_1623140774028_0008\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1623140774028_0008\n",
      "  The url to track the job: http://bdcc:8088/proxy/application_1623140774028_0008/\n",
      "  Running job: job_1623140774028_0008\n",
      "  Job job_1623140774028_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1623140774028_0008 completed successfully\n",
      "  Output directory: hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210608.112139.337160/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=38724796\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1311\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17116132\n",
      "\t\tFILE: Number of bytes written=34927431\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=38725098\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=1311\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=131019776\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15836160\n",
      "\t\tTotal time spent by all map tasks (ms)=127949\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=127949\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15465\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15465\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=127949\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=15465\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=44890\n",
      "\t\tCombine input records=2987624\n",
      "\t\tCombine output records=1309682\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=542\n",
      "\t\tInput split bytes=302\n",
      "\t\tMap input records=2987624\n",
      "\t\tMap output bytes=32352124\n",
      "\t\tMap output materialized bytes=17116138\n",
      "\t\tMap output records=2987624\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=251252736\n",
      "\t\tPeak Map Virtual memory (bytes)=2527506432\n",
      "\t\tPeak Reduce Physical memory (bytes)=130899968\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2488623104\n",
      "\t\tPhysical memory (bytes) snapshot=586268672\n",
      "\t\tReduce input groups=1134140\n",
      "\t\tReduce input records=1309682\n",
      "\t\tReduce output records=100\n",
      "\t\tReduce shuffle bytes=17116138\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=2619364\n",
      "\t\tTotal committed heap usage (bytes)=398663680\n",
      "\t\tVirtual memory (bytes) snapshot=7456776192\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6219206943008186485/] [] /tmp/streamjob6165491951858516627.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bdccuser/.staging/job_1623140774028_0009\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Total input files to process : 1\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  number of splits:2\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Submitting tokens for job: job_1623140774028_0009\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1623140774028_0009\n",
      "  The url to track the job: http://bdcc:8088/proxy/application_1623140774028_0009/\n",
      "  Running job: job_1623140774028_0009\n",
      "  Job job_1623140774028_0009 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1623140774028_0009 completed successfully\n",
      "  Output directory: hdfs:///user/bdccuser/task3_output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1967\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1311\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2417\n",
      "\t\tFILE: Number of bytes written=698580\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2293\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=1311\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=21326848\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5624832\n",
      "\t\tTotal time spent by all map tasks (ms)=20827\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=20827\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5493\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5493\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=20827\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5493\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1970\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=496\n",
      "\t\tInput split bytes=326\n",
      "\t\tMap input records=100\n",
      "\t\tMap output bytes=2211\n",
      "\t\tMap output materialized bytes=2423\n",
      "\t\tMap output records=100\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=224710656\n",
      "\t\tPeak Map Virtual memory (bytes)=2481979392\n",
      "\t\tPeak Reduce Physical memory (bytes)=120242176\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2488348672\n",
      "\t\tPhysical memory (bytes) snapshot=569163776\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=100\n",
      "\t\tReduce shuffle bytes=2423\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=200\n",
      "\t\tTotal committed heap usage (bytes)=398663680\n",
      "\t\tVirtual memory (bytes) snapshot=7452307456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/bdccuser/task3_output\n",
      "Removing HDFS temp directory hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210608.112139.337160...\n",
      "Removing temp directory /tmp/task3.bdccuser.20210608.112139.337160...\n"
     ]
    }
   ],
   "source": [
    "!python3 src/task3.py -r hadoop data/graph.txt -o task3_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the output from HDFS to local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-08 14:24:43,242 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -copyToLocal task3_output /home/bdccuser/bdcc-assignment1/output/task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
