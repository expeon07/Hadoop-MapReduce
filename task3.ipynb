{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDCC HW: Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Implement a MapReduce job that identifies the 100 most followed users in the dataset.\n",
    "\n",
    "Hint: We are not interested in creating lists of followers here. We just need to count the followers of each user in the Reduce phase. This is called the in-degree of a user. Moreover, a temporary data structure D of fixed 100 positions is required. This structure will be initially filled with the first 100 users that are processed in the Reduce phase. Then, the next users (101, 102, 103…) will replace a user in D, only if their in-degree is greater than the in-degree of the least-followed user in D. Notice that the ideal data structure for D is a min-heap (https://docs.python.org/3/library/heapq.html). However, it is totally acceptable to appropriately use a data structure such as a dictionary, or a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/task3.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/task3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import heapq\n",
    "\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "TOP_FOLLOWERS = 100\n",
    "\n",
    "# Implement a MapReduce job that creates a list of followees for each user in the dataset.\n",
    "class MostFollowed(MRJob):\n",
    "\n",
    "    # Arg 1: self: the class itself (this)\n",
    "    # Arg 2: Input key to the map function (here:none)\n",
    "    # Arg 3: Input value to the map function (here:one line from the input file)\n",
    "    def mapper(self, _, line):\n",
    "        # yield (followee, 1) pair\n",
    "        (follower, followee) = line.split()\n",
    "        yield(followee, 1)\n",
    "\n",
    "\n",
    "    def combiner(self, followee, follower_count):\n",
    "        # yield sum of followers\n",
    "        yield(followee, sum(follower_count))\n",
    "\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.heap = []\n",
    "\n",
    "    # Arg 1: self: the class itself (this)\n",
    "    # Arg 2: Input key to the reduce function (here: the key that was emitted by the mapper)\n",
    "    # Arg 3: Input value to the reduce function (here: a generator object; something like a\n",
    "    # sorted list of ALL values associated with the same key)\n",
    "    def reducer(self, followee, follower_count):\n",
    "        heapq.heappush(self.heap, (sum(follower_count), followee))\n",
    "        \n",
    "        if len(self.heap) > TOP_FOLLOWERS:\n",
    "            heapq.heappop(self.heap)\n",
    "\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for (follower_count, followee) in self.heap:\n",
    "            yield (followee, follower_count)\n",
    "\n",
    "\n",
    "    # Step 2 — The global TOP_FOLLOWERS needs to run.\n",
    "    # The mapper outputs \"TOP_FOLLOWERS\" as the key and (count,word) as the value.\n",
    "    # We put the count first so that it can be used directly as input to heapq.nlargest()\n",
    "    def globalTop_mapper(self, followee, follower_count):\n",
    "        yield \"Top \" + str(TOP_FOLLOWERS), (follower_count, followee)\n",
    "\n",
    "\n",
    "    # The reducer ignores the key (\"TOP_FOLLOWERS\"), \n",
    "    # and just finds the largest of the values.\n",
    "    def globalTop_reducer(self ,_, follower_count):\n",
    "        for follower_count in heapq.nlargest(TOP_FOLLOWERS, follower_count):\n",
    "            yield follower_count[1], follower_count[0]\n",
    "\n",
    "\n",
    "    # TODO count may be wrong\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final\n",
    "                   ),\n",
    "\n",
    "            MRStep(mapper=self.globalTop_mapper,\n",
    "                   reducer=self.globalTop_reducer) \n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostFollowed.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in Standalone Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task3.bdccuser.20210601.193021.664354\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task3.bdccuser.20210601.193021.664354/output\n",
      "Streaming final output from /tmp/task3.bdccuser.20210601.193021.664354/output...\n",
      "\"663931\"\t4256\n",
      "\"482709\"\t3137\n",
      "\"663560\"\t2602\n",
      "\"357531\"\t1970\n",
      "\"1034018\"\t1599\n",
      "\"664789\"\t1592\n",
      "\"663606\"\t1491\n",
      "\"155432\"\t963\n",
      "\"622420\"\t956\n",
      "\"280436\"\t907\n",
      "\"115241\"\t871\n",
      "\"665990\"\t836\n",
      "\"664320\"\t828\n",
      "\"115674\"\t812\n",
      "\"1120568\"\t798\n",
      "\"650922\"\t787\n",
      "\"666763\"\t785\n",
      "\"681308\"\t716\n",
      "\"402217\"\t698\n",
      "\"515984\"\t692\n",
      "\"108624\"\t664\n",
      "\"682469\"\t663\n",
      "\"681398\"\t639\n",
      "\"670310\"\t625\n",
      "\"682734\"\t618\n",
      "\"663683\"\t609\n",
      "\"280408\"\t607\n",
      "\"670353\"\t576\n",
      "\"655586\"\t572\n",
      "\"663571\"\t548\n",
      "\"401877\"\t543\n",
      "\"667359\"\t538\n",
      "\"670923\"\t521\n",
      "\"1032030\"\t503\n",
      "\"17405\"\t501\n",
      "\"663852\"\t499\n",
      "\"663534\"\t499\n",
      "\"656427\"\t499\n",
      "\"670279\"\t494\n",
      "\"254927\"\t492\n",
      "\"1095899\"\t491\n",
      "\"667116\"\t481\n",
      "\"642336\"\t465\n",
      "\"678519\"\t458\n",
      "\"663579\"\t458\n",
      "\"480826\"\t446\n",
      "\"105478\"\t428\n",
      "\"686285\"\t422\n",
      "\"697499\"\t417\n",
      "\"1030566\"\t404\n",
      "\"1108861\"\t401\n",
      "\"1014440\"\t400\n",
      "\"666986\"\t399\n",
      "\"663619\"\t378\n",
      "\"685162\"\t377\n",
      "\"1067598\"\t375\n",
      "\"1064471\"\t364\n",
      "\"683704\"\t355\n",
      "\"663548\"\t350\n",
      "\"656431\"\t350\n",
      "\"666653\"\t345\n",
      "\"682680\"\t337\n",
      "\"323007\"\t331\n",
      "\"681748\"\t330\n",
      "\"114922\"\t330\n",
      "\"665647\"\t328\n",
      "\"707743\"\t323\n",
      "\"93931\"\t314\n",
      "\"62876\"\t311\n",
      "\"639586\"\t310\n",
      "\"94027\"\t308\n",
      "\"23463\"\t307\n",
      "\"1024068\"\t305\n",
      "\"683057\"\t302\n",
      "\"670343\"\t302\n",
      "\"666666\"\t301\n",
      "\"8792\"\t299\n",
      "\"664242\"\t293\n",
      "\"681623\"\t289\n",
      "\"199209\"\t287\n",
      "\"672641\"\t285\n",
      "\"106355\"\t285\n",
      "\"20934\"\t284\n",
      "\"1020498\"\t281\n",
      "\"683230\"\t280\n",
      "\"44006\"\t280\n",
      "\"666758\"\t279\n",
      "\"111468\"\t279\n",
      "\"1038591\"\t279\n",
      "\"684652\"\t278\n",
      "\"667325\"\t278\n",
      "\"716475\"\t276\n",
      "\"1099803\"\t275\n",
      "\"666717\"\t271\n",
      "\"21352\"\t270\n",
      "\"682140\"\t265\n",
      "\"705285\"\t264\n",
      "\"268337\"\t264\n",
      "\"1021166\"\t263\n",
      "\"666701\"\t262\n",
      "Removing temp directory /tmp/task3.bdccuser.20210601.193021.664354...\n"
     ]
    }
   ],
   "source": [
    "!python3 src/task3.py data/graph.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/hdoop/hadoop-3.2.1/bin...\n",
      "Found hadoop binary: /home/hdoop/hadoop-3.2.1/bin/hadoop\n",
      "Using Hadoop version 3.2.1\n",
      "Looking for Hadoop streaming jar in /home/hdoop/hadoop-3.2.1...\n",
      "Found Hadoop streaming jar: /home/hdoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar\n",
      "Creating temp directory /tmp/task3.bdccuser.20210601.193148.568766\n",
      "uploading working dir files to hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210601.193148.568766/files/wd...\n",
      "Copying other local files to hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210601.193148.568766/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8780148335038556334/] [] /tmp/streamjob7161871160128299185.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bdccuser/.staging/job_1622571802227_0004\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Total input files to process : 1\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  number of splits:2\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Submitting tokens for job: job_1622571802227_0004\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1622571802227_0004\n",
      "  The url to track the job: http://bdcc:8088/proxy/application_1622571802227_0004/\n",
      "  Running job: job_1622571802227_0004\n",
      "  Job job_1622571802227_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1622571802227_0004_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "  Task Id : attempt_1622571802227_0004_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "  Task Id : attempt_1622571802227_0004_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "  Task Id : attempt_1622571802227_0004_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "  Task Id : attempt_1622571802227_0004_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "  Task Id : attempt_1622571802227_0004_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1622571802227_0004 failed with state FAILED due to: Task failed task_1622571802227_0004_m_000001\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0\n",
      "\n",
      "  Job not successful!\n",
      "  Streaming Command Failed!\n",
      "Counters: 14\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tFailed map tasks=7\n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tOther local map tasks=6\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=121756672\n",
      "\t\tTotal time spent by all map tasks (ms)=118903\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=118903\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=118903\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "Scanning logs for probable cause of failure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for history log in hdfs:///tmp/hadoop-yarn/staging...\n",
      "Looking for history log in /home/hdoop/hadoop-3.2.1/logs...\n",
      "Looking for task syslogs in /home/hdoop/hadoop-3.2.1/logs/userlogs/application_1622571802227_0004...\n",
      "\n",
      "Probable cause of failure:\n",
      "\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "\n",
      "Step 1 of 2 failed: Command '['/home/hdoop/hadoop-3.2.1/bin/hadoop', 'jar', '/home/hdoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar', '-files', 'hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210601.193148.568766/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210601.193148.568766/files/wd/setup-wrapper.sh#setup-wrapper.sh,hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210601.193148.568766/files/wd/task3.py#task3.py', '-input', 'hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210601.193148.568766/files/graph.txt', '-output', 'hdfs:///user/bdccuser/tmp/mrjob/task3.bdccuser.20210601.193148.568766/step-output/0000', '-mapper', '/bin/sh -ex setup-wrapper.sh python3 task3.py --step-num=0 --mapper', '-combiner', '/bin/sh -ex setup-wrapper.sh python3 task3.py --step-num=0 --combiner', '-reducer', '/bin/sh -ex setup-wrapper.sh python3 task3.py --step-num=0 --reducer']' returned non-zero exit status 256.\n"
     ]
    }
   ],
   "source": [
    "!python3 src/task3.py -r hadoop data/graph.txt -o task3_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the output from HDFS to local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -copyToLocal task3_output /home/bdccuser/bdcc-assignment1/output/task3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
