{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python364jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.6.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# BDCC HW: Task 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Task: Implement a MapReduce job that identifies the 100 most followed users in the dataset.\n",
    "\n",
    "Hint: We are not interested in creating lists of followers here. We just need to count the followers of each user in the Reduce phase. This is called the in-degree of a user. Moreover, a temporary data structure D of fixed 100 positions is required. This structure will be initially filled with the first 100 users that are processed in the Reduce phase. Then, the next users (101, 102, 103…) will replace a user in D, only if their in-degree is greater than the in-degree of the least-followed user in D. Notice that the ideal data structure for D is a min-heap (https://docs.python.org/3/library/heapq.html). However, it is totally acceptable to appropriately use a data structure such as a dictionary, or a list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting src/task3.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/task3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import heapq\n",
    "\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "TOP_FOLLOWERS = 100\n",
    "\n",
    "# Implement a MapReduce job that creates a list of followees for each user in the dataset.\n",
    "class MostFollowed(MRJob):\n",
    "\n",
    "    # Arg 1: self: the class itself (this)\n",
    "    # Arg 2: Input key to the map function (here:none)\n",
    "    # Arg 3: Input value to the map function (here:one line from the input file)\n",
    "    def mapper(self, _, line):\n",
    "        # yield (followee, 1) pair\n",
    "        (follower, followee) = line.split()\n",
    "        yield(followee, 1)\n",
    "\n",
    "\n",
    "    def combiner(self, followee, follower_count):\n",
    "\n",
    "        # yield sum of followers\n",
    "        yield(followee, sum(follower_count))\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.heap = []\n",
    "\n",
    "    # Arg 1: self: the class itself (this)\n",
    "    # Arg 2: Input key to the reduce function (here: the key that was emitted by the mapper)\n",
    "    # Arg 3: Input value to the reduce function (here: a generator object; something like a\n",
    "    # sorted list of ALL values associated with the same key)\n",
    "    def reducer(self, followee, follower_count):\n",
    "        heapq.heappush(self.heap, (sum(follower_count), followee))\n",
    "        \n",
    "        if len(self.heap) > TOP_FOLLOWERS:\n",
    "            heapq.heappop(self.heap)\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for (follower_count, followee) in self.heap:\n",
    "            yield (followee, follower_count)\n",
    "\n",
    "\n",
    "    # Step 2 — The global TOP_FOLLOWERS needs to run.\n",
    "    # The mapper outputs \"TOP_FOLLOWERS\" as the key and (count,word) as the value.\n",
    "    # We put the count first so that it can be used directly as input to heapq.nlargest()\n",
    "    def globalTop_mapper(self, followee, follower_count):\n",
    "        yield \"Top \" + str(TOP_FOLLOWERS), (follower_count, followee)\n",
    "\n",
    "\n",
    "    # The reducer ignores the key (\"TOP_FOLLOWERS\"), \n",
    "    # and just finds the largest of the values.\n",
    "    def globalTop_reducer(self ,_, follower_count):\n",
    "        for follower_count in heapq.nlargest(TOP_FOLLOWERS, follower_count):\n",
    "            yield follower_count[1], follower_count[0]\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final\n",
    "                   ),\n",
    "\n",
    "            MRStep(mapper=self.globalTop_mapper,\n",
    "                   reducer=self.globalTop_reducer) \n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostFollowed.run()"
   ]
  },
  {
   "source": [
    "### Run in Standalone Mode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/f7/py_w9f_n5fz4vg7_hty30b0m0000gn/T/task3.sheenafernandez.20210519.203332.109920\n",
      "Running step 1 of 1...\n",
      "\n",
      "Error while reading from /var/folders/f7/py_w9f_n5fz4vg7_hty30b0m0000gn/T/task3.sheenafernandez.20210519.203332.109920/step/000/reducer/00000/input:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"src/task3.py\", line 53, in <module>\n",
      "    MostFollowed.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/job.py\", line 616, in run\n",
      "    cls().execute()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/job.py\", line 687, in execute\n",
      "    self.run_job()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/job.py\", line 636, in run_job\n",
      "    runner.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/runner.py\", line 503, in run\n",
      "    self._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/sim.py\", line 161, in _run\n",
      "    self._run_step(step, step_num)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/sim.py\", line 170, in _run_step\n",
      "    self._run_streaming_step(step, step_num)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/sim.py\", line 187, in _run_streaming_step\n",
      "    self._run_reducers(step_num, num_reducer_tasks)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/sim.py\", line 291, in _run_reducers\n",
      "    for task_num in range(num_reducer_tasks)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/sim.py\", line 130, in _run_multiple\n",
      "    func()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/sim.py\", line 747, in _run_task\n",
      "    stdin, stdout, stderr, wd, env)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/inline.py\", line 133, in invoke_task\n",
      "    task.execute()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/job.py\", line 681, in execute\n",
      "    self.run_reducer(self.options.step_num)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/job.py\", line 795, in run_reducer\n",
      "    for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/job.py\", line 866, in reduce_pairs\n",
      "    for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/job.py\", line 889, in _combine_or_reduce_pairs\n",
      "    for k, v in task(key, values) or ():\n",
      "  File \"src/task3.py\", line 49, in reducer\n",
      "    yield(top_followed[0], top_followed[1])\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!python3 src/task3.py data/graph.txt"
   ]
  },
  {
   "source": [
    "### Run in the Hadoop cluster in a fully/pseudo distributed mode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No configs found; falling back on auto-configuration\nNo configs specified for hadoop runner\nLooking for hadoop binary in $PATH...\nFalling back to 'hadoop'\nTraceback (most recent call last):\n  File \"src/task1.py\", line 31, in <module>\n    Followers.run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/job.py\", line 616, in run\n    cls().execute()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/job.py\", line 687, in execute\n    self.run_job()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/job.py\", line 636, in run_job\n    runner.run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/runner.py\", line 503, in run\n    self._run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/hadoop.py\", line 325, in _run\n    self._find_binaries_and_jars()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/hadoop.py\", line 339, in _find_binaries_and_jars\n    self.get_hadoop_version()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/hadoop.py\", line 210, in get_hadoop_version\n    return self.fs.hadoop.get_hadoop_version()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/fs/hadoop.py\", line 128, in get_hadoop_version\n    stdout = self.invoke_hadoop(['version'], return_stdout=True)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mrjob/fs/hadoop.py\", line 160, in invoke_hadoop\n    proc = Popen(args, stdout=PIPE, stderr=PIPE)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/subprocess.py\", line 709, in __init__\n    restore_signals, start_new_session)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/subprocess.py\", line 1344, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nFileNotFoundError: [Errno 2] No such file or directory: 'hadoop': 'hadoop'\n"
     ]
    }
   ],
   "source": [
    "!python3 src/task3.py -r hadoop data/graph.txt -o task3_output"
   ]
  },
  {
   "source": [
    "### Copy the output from HDFS to local file system."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -copyToLocal task2_output /home/bdccuser/bdcc-assignment1/output/task2"
   ]
  }
 ]
}